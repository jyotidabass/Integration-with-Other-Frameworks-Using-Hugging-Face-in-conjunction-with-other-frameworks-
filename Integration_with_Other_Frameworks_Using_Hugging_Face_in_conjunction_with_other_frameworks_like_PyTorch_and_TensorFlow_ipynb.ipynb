{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoF2zfEyGMPOmmHSxCMJTG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jyotidabass/Integration-with-Other-Frameworks-Using-Hugging-Face-in-conjunction-with-other-frameworks-/blob/main/Integration_with_Other_Frameworks_Using_Hugging_Face_in_conjunction_with_other_frameworks_like_PyTorch_and_TensorFlow_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hugging Face with PyTorch**"
      ],
      "metadata": {
        "id": "3qEh8RONuhJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define a PyTorch dataset class\n",
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        labels = self.labels[idx]\n",
        "\n",
        "        # Preprocess text using Hugging Face tokenizer\n",
        "        inputs = tokenizer(text, return_tensors='pt')\n",
        "\n",
        "        # Get BERT embeddings\n",
        "        outputs = model(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
        "\n",
        "        # Return BERT embeddings and labels\n",
        "        return outputs.last_hidden_state[:, 0, :], labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "# Create a sample dataset\n",
        "texts = ['This is a sample text.', 'This is another sample text.']\n",
        "labels = [0, 1]\n",
        "\n",
        "dataset = MyDataset(texts, labels)\n",
        "\n",
        "# Create a PyTorch data loader\n",
        "batch_size = 32  # Though set to 32, it will process 2 samples at a time in this case\n",
        "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Train a PyTorch model using Hugging Face BERT embeddings\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Renamed to 'classifier' to avoid overwriting the BERT 'model'\n",
        "classifier = torch.nn.Linear(768, 2)  # 768 is the dimensionality of BERT embeddings\n",
        "classifier.to(device) # Send to the device\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "# Updated optimizer to use classifier's parameters\n",
        "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-5)\n",
        "\n",
        "for epoch in range(5):\n",
        "    for batch in data_loader:\n",
        "        inputs, labels = batch\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass - pass only embeddings to the linear layer, remove extra dimension\n",
        "        outputs = classifier(inputs.squeeze(1))\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update model parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16u8HKuVwVVK",
        "outputId": "1dd03525-686c-491d-ecc2-9e7376629b92"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.6920924782752991\n",
            "Epoch 2, Loss: 0.6914633512496948\n",
            "Epoch 3, Loss: 0.6908381581306458\n",
            "Epoch 4, Loss: 0.6902168989181519\n",
            "Epoch 5, Loss: 0.6895996332168579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hugging Face with TensorFlow**"
      ],
      "metadata": {
        "id": "usd95drquorj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model_bert = TFBertModel.from_pretrained('bert-base-uncased')  # Renamed to avoid confusion\n",
        "\n",
        "# Define a TensorFlow dataset class\n",
        "class MyDataset:  # Inherit from object instead of tf.data.Dataset\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "        # Create a tf.data.Dataset from the text and labels\n",
        "        self.dataset = tf.data.Dataset.from_tensor_slices((self.texts, self.labels))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        labels = self.labels[idx]\n",
        "\n",
        "        # Preprocess text using Hugging Face tokenizer\n",
        "        inputs = tokenizer(text, return_tensors='tf')\n",
        "\n",
        "        # Get BERT embeddings\n",
        "        outputs = model_bert(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
        "\n",
        "        # Return BERT embeddings and labels\n",
        "        return outputs.last_hidden_state[:, 0, :], labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def get_dataset(self):\n",
        "        \"\"\"Returns the tf.data.Dataset object.\"\"\"\n",
        "        return self.dataset\n",
        "\n",
        "\n",
        "# Create a sample dataset\n",
        "texts = ['This is a sample text.', 'This is another sample text.']\n",
        "labels = [0, 1]\n",
        "\n",
        "dataset = MyDataset(texts, labels)\n",
        "\n",
        "# Create a TensorFlow data loader using the dataset's get_dataset method\n",
        "batch_size = 32\n",
        "data_loader = dataset.get_dataset().batch(batch_size)  # Modified\n",
        "\n",
        "# Train a TensorFlow model using Hugging Face BERT embeddings\n",
        "# Define the model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input(shape=(768,)),  # Input layer for BERT embeddings\n",
        "    tf.keras.layers.Dense(2, activation='softmax')  # Output layer with 2 classes\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(5):\n",
        "    for batch in data_loader:\n",
        "        inputs, labels = batch\n",
        "\n",
        "        # Preprocess inputs using Hugging Face tokenizer\n",
        "        inputs = tokenizer(inputs.numpy().astype(str).tolist(), return_tensors='tf', padding=True, truncation=True)\n",
        "\n",
        "        # Get BERT embeddings using the TFBertModel (model_bert)\n",
        "        with tf.GradientTape() as tape:\n",
        "            embeddings = model_bert(inputs['input_ids'], attention_mask=inputs['attention_mask']).last_hidden_state[:, 0, :]\n",
        "\n",
        "            # Get the classification output using the Sequential model (model)\n",
        "            outputs = model(embeddings)\n",
        "            loss = tf.keras.losses.sparse_categorical_crossentropy(labels, outputs)\n",
        "\n",
        "        # Calculate gradients\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "        # Update model weights\n",
        "        model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "        print(f'Epoch {epoch + 1}, Loss: {loss.numpy().mean()}, Accuracy: {tf.keras.metrics.sparse_categorical_accuracy(labels, outputs).numpy().mean()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcBakHlfxTup",
        "outputId": "20781110-955e-4c51-a2da-4120a580d3a4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.7286123037338257, Accuracy: 0.5\n",
            "Epoch 2, Loss: 0.6961890459060669, Accuracy: 0.5\n",
            "Epoch 3, Loss: 0.6651514768600464, Accuracy: 1.0\n",
            "Epoch 4, Loss: 0.6350632905960083, Accuracy: 1.0\n",
            "Epoch 5, Loss: 0.6060135364532471, Accuracy: 1.0\n"
          ]
        }
      ]
    }
  ]
}